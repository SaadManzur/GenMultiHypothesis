{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import src.cameras as cameras\n",
    "from tensorflow.python.client import device_lib\n",
    "from src.mix_den_model import LinearModel\n",
    "from src.data_utils import postprocess_3d, define_actions, project_to_cameras\n",
    "from src.data_utils import transform_world_to_camera, postprocess_3d, normalization_stats, normalize_data\n",
    "from src.data_utils import unNormalizeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = {}\n",
    "FLAGS[\"learning_rate\"] = 1e-3 # \"Learning rate\"\n",
    "FLAGS[\"dropout\"] = 0.5 # \"Dropout keep probability. 1 means no dropout\"\n",
    "FLAGS[\"batch_size\"] = 64 #\"batch size to use during training\")\n",
    "FLAGS[\"epochs\"] = 200 # \"How many epochs we should train for\")\n",
    "FLAGS[\"camera_frame\"] = True # \"Convert 3d poses to camera coordinates\")\n",
    "FLAGS[\"max_norm\"] = True # \"Apply maxnorm constraint to the weights\")\n",
    "FLAGS[\"batch_norm\"] = True # \"Use batch_normalization\")\n",
    "\n",
    "# Data loading\n",
    "FLAGS[\"predict_14\"] = False # \"predict 14 joints\")\n",
    "FLAGS[\"use_sh\"] = True # \"Use 2d pose predictions from StackedHourglass\")\n",
    "FLAGS[\"action\"] = \"All\" # \"The action to train on. 'All' means all the actions\")\n",
    "\n",
    "# Architecture\n",
    "FLAGS[\"linear_size\"] = 1024 # \"Size of each model layer.\")\n",
    "FLAGS[\"num_layers\"] = 2 # \"Number of layers in the model.\")\n",
    "FLAGS[\"residual\"] = True # \"Whether to add a residual connection every 2 layers\")\n",
    "\n",
    "# Evaluation\n",
    "FLAGS[\"procrustes\"] = False # \"Apply procrustes analysis at test time\")\n",
    "FLAGS[\"evaluateActionWise\"] = True # \"The dataset to use either h36m or heva\")\n",
    "\n",
    "# Directories\n",
    "FLAGS[\"cameras_path\"] = \"../data/h36m/cameras.h5\" # \"Directory to load camera parameters\")\n",
    "FLAGS[\"data_dir\"] = \"../data/h36m/\" # \"Data directory\")\n",
    "FLAGS[\"train_dir\"] = \"../experiments/test_git/\" # \"Training directory.\")\n",
    "FLAGS[\"load_dir\"] = \"../Models/mdm_5_prior/\" # \"Specify the directory to load trained model\")\n",
    "\n",
    "# Train or load\n",
    "FLAGS[\"sample\"] = False # \"Set to True for sampling.\")\n",
    "FLAGS[\"test\"] = False # \"Set to True for sampling.\")\n",
    "FLAGS[\"use_cpu\"] = False # \"Whether to use the CPU\")\n",
    "FLAGS[\"load\"] = 3192601 # \"Try to load a previous checkpoint.\")\n",
    "FLAGS[\"miss_num\"] = 1 # \"Specify how many missing joints.\")\n",
    "\n",
    "### 4679232 for mdm_5\n",
    "### 4338038 for mdm prior\n",
    "\n",
    "# Misc\n",
    "FLAGS[\"use_fp16\"] = False # \"Train using fp16 instead of fp32.\")\n",
    "FLAGS[\"n_joints\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS[\"load_dir\"] = \"experiments/first_May13/\"\n",
    "\n",
    "summaries_dir = os.path.join(FLAGS[\"train_dir\"], \"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_stats(complete_data):\n",
    "\n",
    "    data_mean = np.mean(complete_data, axis=0)\n",
    "    data_std  = np.std(complete_data, axis=0)\n",
    "\n",
    "    return data_mean, data_std\n",
    "\n",
    "def normalize_data(data, mu, stddev):\n",
    "\n",
    "    if data.shape[-1]//FLAGS['n_joints'] == 3:\n",
    "        new_data = data\n",
    "        new_data[:, 3:] = np.divide((data[:, 3:] - mu[3:]), stddev[3:] )\n",
    "        return new_data\n",
    "\n",
    "    return np.divide( (data - mu), stddev )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unNormalizeData(normalized_data, data_mean, data_std):\n",
    "    T = normalized_data.shape[0] # Batch size\n",
    "    D = data_mean.shape[0] # Dimensionality\n",
    "\n",
    "    orig_data = np.zeros((T, D), dtype=np.float32)\n",
    "\n",
    "    orig_data[:, :] = normalized_data\n",
    "\n",
    "    # Multiply times stdev and add the mean\n",
    "    stdMat = data_std.reshape((1, D))\n",
    "    stdMat = np.repeat(stdMat, T, axis=0)\n",
    "    meanMat = data_mean.reshape((1, D))\n",
    "    meanMat = np.repeat(meanMat, T, axis=0)\n",
    "    orig_data[:, 3:] = np.multiply(orig_data[:, 3:], stdMat[:, 3:]) + meanMat[:, 3:]\n",
    "    return orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h36m_dataset(bpath, subjects, actions, dim):\n",
    "\n",
    "    indices_to_select = [0, 1, 2, 3, 6, 7, 8, 12, 13, 15, 17, 18, 19, 25, 26, 27]\n",
    "\n",
    "    if not dim in [2,3]:\n",
    "        raise(ValueError, 'dim must be 2 or 3')\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for subj in subjects:\n",
    "        for action in actions:\n",
    "\n",
    "            print('Reading subject {0}, action {1}'.format(subj, action))\n",
    "\n",
    "            dpath = os.path.join( bpath, 'S{0}'.format(subj), 'MyPoses/{0}D_positions'.format(dim), '{0}*.h5'.format(action) )\n",
    "            print( dpath )\n",
    "\n",
    "            fnames = glob.glob( dpath )\n",
    "\n",
    "            loaded_seqs = 0\n",
    "            for fname in fnames:\n",
    "                seqname = os.path.basename( fname )\n",
    "\n",
    "                # This rule makes sure SittingDown is not loaded when Sitting is requested\n",
    "                if action == \"Sitting\" and seqname.startswith( \"SittingDown\" ):\n",
    "                    continue\n",
    "\n",
    "                # This rule makes sure that WalkDog and WalkTogeter are not loaded when\n",
    "                # Walking is requested.\n",
    "                if seqname.startswith( action ):\n",
    "                    print( fname )\n",
    "                    loaded_seqs = loaded_seqs + 1\n",
    "\n",
    "                with h5py.File( fname, 'r' ) as h5f:\n",
    "                    poses = np.array(h5f['{0}D_positions'.format(dim)][:])\n",
    "                    # poses = poses.reshape((32, 3, poses.shape[1]))\n",
    "                    # poses = poses[indices_to_select, :, :]\n",
    "                    # poses = poses.reshape((48, poses.shape[2]))\n",
    "\n",
    "                poses = poses.T\n",
    "                data[ (subj, action, seqname) ] = poses\n",
    "\n",
    "        if dim == 2:\n",
    "            assert loaded_seqs == 8, \"Expecting 8 sequences, found {0} instead\".format( loaded_seqs )\n",
    "        else:\n",
    "            assert loaded_seqs == 2, \"Expecting 2 sequences, found {0} instead\".format( loaded_seqs )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(session, batch_size):\n",
    "\n",
    "    model = LinearModel(\n",
    "      FLAGS[\"linear_size\"],\n",
    "      FLAGS[\"num_layers\"],\n",
    "      FLAGS[\"residual\"],\n",
    "      FLAGS[\"batch_norm\"],\n",
    "      FLAGS[\"max_norm\"],\n",
    "      batch_size,\n",
    "      FLAGS[\"learning_rate\"],\n",
    "      summaries_dir,\n",
    "      FLAGS[\"predict_14\"],\n",
    "      dtype=tf.float16 if FLAGS[\"use_fp16\"] else tf.float32)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS[\"load_dir\"], latest_filename=\"checkpoint\")\n",
    "\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        if FLAGS[\"load\"] > 0:\n",
    "            if os.path.isfile(os.path.join(FLAGS[\"load_dir\"],\"checkpoint-{0}.index\".format(FLAGS[\"load\"]))):\n",
    "                ckpt_name = os.path.join( os.path.join(FLAGS[\"load_dir\"],\"checkpoint-{0}\".format(FLAGS[\"load\"])) )\n",
    "            else:\n",
    "                raise ValueError(\"Asked to load checkpoint {0}, but it does not seem to exist\".format(FLAGS[\"load\"]))\n",
    "        else:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        \n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        return model\n",
    "    else:\n",
    "        raise( ValueError, \"Checkpoint {0} does not seem to exist\".format( ckpt.model_checkpoint_path ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batches( sess, model, data_mean_3d, data_std_3d, data_mean_2d, data_std_2d,\n",
    "    encoder_inputs, decoder_outputs, current_epoch=0 ):\n",
    "\n",
    "    nbatches = len( encoder_inputs )\n",
    "    n_joints = 16\n",
    "\n",
    "    # Loop through test examples\n",
    "    all_dists, start_time, loss = [], time.time(), 0.0\n",
    "    all_poses_3d = []\n",
    "    all_enc_in =[]\n",
    "\n",
    "    for i in range(nbatches):\n",
    "\n",
    "        if current_epoch > 0 and (i+1) % log_every_n_batches == 0:\n",
    "            print(\"Working on test epoch {0}, batch {1} / {2}\".format( current_epoch, i+1, nbatches) )\n",
    "\n",
    "        enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "        # enc_in = data_utils.generage_missing_data(enc_in, FLAGS.miss_num)\n",
    "        dp = 1.0 # dropout keep probability is always 1 at test time\n",
    "        step_loss, loss_summary, out_all_components_ori = model.step( sess, enc_in, dec_out, dp, isTraining=False )\n",
    "        loss += step_loss\n",
    "\n",
    "        out_all_components = np.reshape(out_all_components_ori,[-1, model.HUMAN_3D_SIZE+2, model.num_models])\n",
    "        out_mean = out_all_components[:, : model.HUMAN_3D_SIZE, :]\n",
    "\n",
    "\n",
    "        # denormalize\n",
    "        enc_in  = unNormalizeData( enc_in,  data_mean_2d, data_std_2d)\n",
    "        enc_in_ = copy.deepcopy(enc_in)\n",
    "        all_enc_in.append(enc_in_)\n",
    "        dec_out = unNormalizeData( dec_out, data_mean_3d, data_std_3d )\n",
    "        pose_3d = np.zeros((enc_in.shape[0], 48, out_mean.shape[-1]))\n",
    "\n",
    "        for j in range(out_mean.shape[-1]):\n",
    "            pose_3d[:, :, j] = unNormalizeData( out_mean[:, :, j], data_mean_3d, data_std_3d )\n",
    "\n",
    "        pose_3d_ = copy.deepcopy(pose_3d)\n",
    "        all_poses_3d.append(pose_3d_)\n",
    "\n",
    "        dec_out = dec_out[:, 3:]\n",
    "        pose_3d = pose_3d[:, 3:, :]\n",
    "\n",
    "        assert dec_out.shape[0] == FLAGS[\"batch_size\"]\n",
    "        assert pose_3d.shape[0] == FLAGS[\"batch_size\"]\n",
    "\n",
    "        # Compute Euclidean distance error per joint\n",
    "        sqerr = (pose_3d - np.expand_dims(dec_out,axis=2))**2 # Squared error between prediction and expected output\n",
    "        dists = np.zeros((sqerr.shape[0], n_joints, sqerr.shape[2])) # Array with L2 error per joint in mm\n",
    "\n",
    "        for m in range(dists.shape[-1]):\n",
    "            dist_idx = 0\n",
    "            for k in np.arange(0, n_joints*3, 3):\n",
    "                # Sum across X,Y, and Z dimenstions to obtain L2 distance\n",
    "                dists[:,dist_idx, m] = np.sqrt( np.sum( sqerr[:, k:k+3,m], axis=1 ))\n",
    "\n",
    "                dist_idx = dist_idx + 1\n",
    "\n",
    "        all_dists.append(dists)\n",
    "        assert sqerr.shape[0] == FLAGS[\"batch_size\"]\n",
    "\n",
    "    step_time = (time.time() - start_time) / nbatches\n",
    "    loss      = loss / nbatches\n",
    "\n",
    "    all_dists = np.vstack( all_dists )\n",
    "    aver_minerr = np.mean(np.min(np.sum( all_dists, axis=1),axis=1))/n_joints\n",
    "\n",
    "    return aver_minerr, step_time, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:81: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:90: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:29: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n\nWARNING:tensorflow:From /home/saad/anaconda3/envs/py36_tf114/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:116: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a045b14a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a045b14a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a045b14a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a045b14a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:118: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a044d7eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a044d7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a044d7eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a044d7eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a043c5e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a043c5e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a043c5e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a043c5e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a0425df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a0425df98>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a0425df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a0425df98>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a040b96d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a040b96d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a040b96d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f4a040b96d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:142: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:379: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:388: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:390: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:159: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:169: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:170: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\nWARNING:tensorflow:From /home/saad/Personal/Research/Experiments/GenMultiHypothesis/src/mix_den_model.py:183: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\nWARNING:tensorflow:From /home/saad/anaconda3/envs/py36_tf114/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\nINFO:tensorflow:Restoring parameters from experiments/first_May13/checkpoint-3192601\n"
    }
   ],
   "source": [
    "device_count = {\"GPU\": 0}\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(device_count=device_count, allow_soft_placement=True))\n",
    "\n",
    "model = create_model(sess, FLAGS[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_2d, data_3d, means, stds):\n",
    "\n",
    "    data_mean_3d, data_std_3d = means[1], stds[1]\n",
    "    data_mean_2d, data_std_2d = means[0], stds[0]\n",
    "\n",
    "    # === Create the model ===\n",
    "    print(\"Creating %d bi-layers of %d units.\" % (FLAGS[\"num_layers\"], FLAGS[\"linear_size\"]))\n",
    "    model.train_writer.add_graph( sess.graph )\n",
    "\n",
    "    cum_err = 0           # select the mixture model which has mininum error\n",
    "    encoder_inputs, decoder_outputs = model.get_all_batches( data_2d, data_3d, FLAGS[\"camera_frame\"], training=False)\n",
    "\n",
    "    act_err, step_time, loss = evaluate_batches(sess, model,\n",
    "                                                data_mean_3d, data_std_3d,\n",
    "                                                data_mean_2d, data_std_2d,\n",
    "                                                encoder_inputs, decoder_outputs)\n",
    "\n",
    "    print(act_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_subset( poses_set, action ):\n",
    "  \"\"\"\n",
    "  Given a preloaded dictionary of poses, load the subset of a particular action\n",
    "\n",
    "  Args\n",
    "    poses_set: dictionary with keys k=(subject, action, seqname),\n",
    "      values v=(nxd matrix of poses)\n",
    "    action: string. The action that we want to filter out\n",
    "  Returns\n",
    "    poses_subset: dictionary with same structure as poses_set, but only with the\n",
    "      specified action.\n",
    "  \"\"\"\n",
    "  return {k:v for k, v in poses_set.items() if k[1] == action}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batches_orig( sess, model,\n",
    "  data_mean_3d, data_std_3d, dim_to_use_3d, dim_to_ignore_3d,\n",
    "  data_mean_2d, data_std_2d, dim_to_use_2d, dim_to_ignore_2d,\n",
    "  current_step, encoder_inputs, decoder_outputs, current_epoch=0 ):\n",
    "  \"\"\"\n",
    "  Generic method that evaluates performance of a list of batches.\n",
    "  May be used to evaluate all actions or a single action.\n",
    "\n",
    "  Args\n",
    "    sess\n",
    "    model\n",
    "    data_mean_3d\n",
    "    data_std_3d\n",
    "    dim_to_use_3d\n",
    "    dim_to_ignore_3d\n",
    "    data_mean_2d\n",
    "    data_std_2d\n",
    "    dim_to_use_2d\n",
    "    dim_to_ignore_2d\n",
    "    current_step\n",
    "    encoder_inputs\n",
    "    decoder_outputs\n",
    "    current_epoch\n",
    "  Returns\n",
    "\n",
    "    total_err\n",
    "    joint_err\n",
    "    step_time\n",
    "    loss\n",
    "  \"\"\"\n",
    "\n",
    "  n_joints = 17 if not(FLAGS[\"predict_14\"]) else 14\n",
    "  nbatches = len( encoder_inputs )\n",
    "\n",
    "\n",
    "  # Loop through test examples\n",
    "  all_dists, start_time, loss = [], time.time(), 0.\n",
    "  log_every_n_batches = 100\n",
    "  all_poses_3d = []\n",
    "  all_enc_in =[]\n",
    "\n",
    "  for i in range(nbatches):\n",
    "\n",
    "    if current_epoch > 0 and (i+1) % log_every_n_batches == 0:\n",
    "      print(\"Working on test epoch {0}, batch {1} / {2}\".format( current_epoch, i+1, nbatches) )\n",
    "\n",
    "    enc_in, dec_out = encoder_inputs[i], decoder_outputs[i]\n",
    "    # enc_in = data_utils.generage_missing_data(enc_in, FLAGS.miss_num)\n",
    "    dp = 1.0 # dropout keep probability is always 1 at test time\n",
    "    step_loss, loss_summary, out_all_components_ori = model.step( sess, enc_in, dec_out, dp, isTraining=False )\n",
    "    loss += step_loss\n",
    "\n",
    "    out_all_components = np.reshape(out_all_components_ori,[-1, model.HUMAN_3D_SIZE+2, model.num_models])\n",
    "    out_mean = out_all_components[:, : model.HUMAN_3D_SIZE, :]\n",
    "\n",
    "\n",
    "    # denormalize\n",
    "    enc_in  = unNormalizeData( enc_in,  data_mean_2d, data_std_2d, dim_to_ignore_2d )\n",
    "    enc_in_ = copy.deepcopy(enc_in)\n",
    "    all_enc_in.append(enc_in_)\n",
    "    dec_out = unNormalizeData( dec_out, data_mean_3d, data_std_3d, dim_to_ignore_3d )\n",
    "    pose_3d = np.zeros((enc_in.shape[0],96, out_mean.shape[-1]))\n",
    "\n",
    "    for j in range(out_mean.shape[-1]):\n",
    "        pose_3d[:, :, j] = unNormalizeData( out_mean[:, :, j], data_mean_3d, data_std_3d, dim_to_ignore_3d )\n",
    "\n",
    "    pose_3d_ = copy.deepcopy(pose_3d)\n",
    "    all_poses_3d.append(pose_3d_)\n",
    "\n",
    "    # Keep only the relevant dimensions\n",
    "    dtu3d = np.hstack( (np.arange(3), dim_to_use_3d) ) if not(FLAGS[\"predict_14\"]) else  dim_to_use_3d\n",
    "\n",
    "    dec_out = dec_out[:, dtu3d]\n",
    "    pose_3d = pose_3d[:, dtu3d,:]\n",
    "\n",
    "    assert dec_out.shape[0] == FLAGS[\"batch_size\"]\n",
    "    assert pose_3d.shape[0] == FLAGS[\"batch_size\"]\n",
    "\n",
    "    if FLAGS[\"procrustes\"]:\n",
    "      # Apply per-frame procrustes alignment if asked to do so\n",
    "      for j in range(FLAGS[\"batch_size\"]):\n",
    "          for k in range(model.num_models):\n",
    "            gt  = np.reshape(dec_out[j,:],[-1,3])\n",
    "            out = np.reshape(pose_3d[j,:, k],[-1,3])\n",
    "            _, Z, T, b, c = procrustes.compute_similarity_transform(gt,out,compute_optimal_scale=True)\n",
    "            out = (b*out.dot(T))+c\n",
    "\n",
    "            pose_3d[j, :, k] = np.reshape(out,[-1,17*3] ) if not(FLAGS[\"predict_14\"]) else np.reshape(pose_3d[j,:, k],[-1,14*3] )\n",
    "\n",
    "    # Compute Euclidean distance error per joint\n",
    "    sqerr = (pose_3d - np.expand_dims(dec_out,axis=2))**2 # Squared error between prediction and expected output\n",
    "    dists = np.zeros((sqerr.shape[0], n_joints, sqerr.shape[2])) # Array with L2 error per joint in mm\n",
    "\n",
    "    for m in range(dists.shape[-1]):\n",
    "      dist_idx = 0\n",
    "      for k in np.arange(0, n_joints*3, 3):\n",
    "        # Sum across X,Y, and Z dimenstions to obtain L2 distance\n",
    "        dists[:,dist_idx, m] = np.sqrt( np.sum( sqerr[:, k:k+3,m], axis=1 ))\n",
    "\n",
    "        dist_idx = dist_idx + 1\n",
    "\n",
    "    all_dists.append(dists)\n",
    "    assert sqerr.shape[0] == FLAGS[\"batch_size\"]\n",
    "\n",
    "  step_time = (time.time() - start_time) / nbatches\n",
    "  loss      = loss / nbatches\n",
    "\n",
    "  all_dists = np.vstack( all_dists )\n",
    "  aver_minerr = np.mean(np.min(np.sum( all_dists, axis=1),axis=1))/n_joints\n",
    "\n",
    "  return aver_minerr, step_time, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_orig(\n",
    "    test_set_2d, test_set_3d,\n",
    "    data_mean_2d, data_mean_3d,\n",
    "    data_std_2d, data_std_3d,\n",
    "    dim_to_use_2d, dim_to_use_3d,\n",
    "    dim_to_ignore_2d, dim_to_ignore_3d):\n",
    "\n",
    "    actions = define_actions( FLAGS[\"action\"] )\n",
    "\n",
    "    if FLAGS[\"evaluateActionWise\"]:\n",
    "\n",
    "      cum_err = 0           # select the mixture model which has mininum error\n",
    "      for action in actions:\n",
    "\n",
    "        # Get 2d and 3d testing data for this action\n",
    "        action_test_set_2d = get_action_subset( test_set_2d, action )\n",
    "        action_test_set_3d = get_action_subset( test_set_3d, action )\n",
    "        encoder_inputs, decoder_outputs = model.get_all_batches( action_test_set_2d, action_test_set_3d, FLAGS[\"camera_frame\"], training=False)\n",
    "\n",
    "        act_err, step_time, loss = evaluate_batches( sess, model,\n",
    "                                                      data_mean_3d, data_std_3d,\n",
    "                                                      data_mean_2d, data_std_2d,\n",
    "                                                      encoder_inputs, decoder_outputs)\n",
    "        \n",
    "        print(action, \" \", act_err)\n",
    "\n",
    "        cum_err = cum_err + act_err\n",
    "\n",
    "    print(cum_err / float(len(actions)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating 2 bi-layers of 1024 units.\n0.3498102956277592\n"
    }
   ],
   "source": [
    "random_data_2d = np.random.rand(64, 32)\n",
    "random_data_3d = np.random.rand(64, 48)\n",
    "\n",
    "evaluate(random_data_2d, random_data_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3DPW Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3dpw = np.load('data/3dpw_combined_actors_v2.npz', allow_pickle=True, encoding='latin1')['data'].item()\n",
    "\n",
    "indices_to_select = [0, 2, 5, 8, 1, 4, 7, 6, 12, 15, 16, 18, 20, 17, 19, 21]\n",
    "\n",
    "data_count =  data_3dpw['test']['combined_3d_cam'].shape[0]\n",
    "\n",
    "data_3d = data_3dpw['test']['combined_3d_cam'][:, indices_to_select, :].reshape((data_count, 48))\n",
    "data_3d = data_3d - np.tile(data_3d[:, :3], [1, 16])\n",
    "data_2d = data_3dpw['test']['combined_2d'][:, indices_to_select, :].reshape((data_count, 32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3d = normalize_data(data_3d, mean_3d, std_3d)\n",
    "data_2d = normalize_data(data_2d, mean_2d, std_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating 2 bi-layers of 1024 units.\n390.89428236198\n"
    }
   ],
   "source": [
    "evaluate(data_2d, data_3d, [mean_2d, mean_3d], [std_2d, std_3d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPA Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating 2 bi-layers of 1024 units.\n173.7848154533636\n"
    }
   ],
   "source": [
    "data_gpa = np.load('data/gpa_xyz_projected_wc_v3.npz', allow_pickle=True, encoding='latin1')['data'].item()\n",
    "\n",
    "indices_to_select = [0, 24, 25, 26, 29, 30, 31, 2, 5, 6, 7, 17, 18, 19, 9, 10, 11]\n",
    "indices_to_sort = [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "data_count = data_gpa['test']['3d'].shape[0]\n",
    "\n",
    "data_3d = data_gpa['test']['3d'][:, indices_to_select, :][:, indices_to_sort, :].reshape((data_count, 48))*10\n",
    "data_3d = data_3d - np.tile(data_3d[:, :3], [1, 16])\n",
    "data_2d = data_gpa['test']['2d_projected'][:, indices_to_select, :][:, indices_to_sort, :].reshape((data_count, 32))\n",
    "\n",
    "evaluate(data_2d, data_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-176.73076784 -321.04861816 5203.88206303]\n [ -52.96191118 -309.7044902  5251.08279046]\n [-155.64155821   73.07175588 5448.80702584]\n [ -29.83157265  506.78445233 5400.13833872]\n [-300.49984317 -332.39276616 5156.68125222]\n [-258.24048857   99.60905053 5244.68147203]\n [-209.48436389  548.83382497 5290.76367197]\n [-109.15762285 -529.7281668  5123.8906273 ]\n [-140.19117593 -780.12137495 5074.60478778]\n [-118.93483197 -970.22827344 5058.5990502 ]\n [-259.08997397 -690.13358895 5050.59205089]\n [-370.67089216 -448.59930095 5134.17726128]\n [-462.28663516 -290.82947286 5307.6274481 ]\n [ -19.76034113 -716.91810277 5140.27255285]\n [  35.79161007 -470.14491849 5257.73845884]\n [  13.89246482 -279.85293245 5421.06854165]]\n[[473.68356 444.9424 ]\n [500.9961  448.02988]\n [479.83926 530.78564]\n [506.21838 622.56885]\n [445.9001  441.81586]\n [456.18906 537.1581 ]\n [467.30923 633.76935]\n [488.18674 397.43405]\n [481.02847 340.39694]\n [485.76895 297.57162]\n [454.01608 359.75955]\n [430.05878 415.7349 ]\n [412.99722 452.88666]\n [508.13437 356.49152]\n [520.3154  413.31827]\n [515.4715  456.42984]]\n"
    }
   ],
   "source": [
    "data_h36m = h5py.File('data/h36m_s1_directions1.h5', 'r')\n",
    "\n",
    "indices_to_select = [0, 1, 2, 3, 6, 7, 8, 12, 13, 15, 17, 18, 19, 25, 26, 27]\n",
    "\n",
    "data_count = data_h36m['pose']['3d-univ'].shape[0]\n",
    "\n",
    "data_3d = np.array(data_h36m['pose']['3d-univ'][:, indices_to_select, :]).reshape((data_count, 48))\n",
    "# data_3d = data_3d - np.tile(data_3d[:, :3], [1, 16])\n",
    "data_2d = np.array(data_h36m['pose']['2d'][:, indices_to_select, :]).reshape((data_count, 32))\n",
    "\n",
    "print(data_3d[0, :].reshape((16, 3)))\n",
    "print(data_2d[0, :].reshape((16, 2)))\n",
    "\n",
    "# evaluate(data_2d, data_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H36M Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "positions/Sitting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Sitting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Sitting 2.h5\nReading subject 6, action SittingDown\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/SittingDown*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/SittingDown 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/SittingDown.h5\nReading subject 6, action Smoking\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Smoking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Smoking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Smoking 1.h5\nReading subject 6, action Waiting\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Waiting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Waiting.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Waiting 3.h5\nReading subject 6, action WalkDog\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkDog*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkDog 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkDog.h5\nReading subject 6, action Walking\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Walking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Walking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/Walking 1.h5\nReading subject 6, action WalkTogether\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkTogether*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkTogether 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S6/MyPoses/3D_positions/WalkTogether.h5\nReading subject 7, action Directions\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Directions*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Directions 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Directions.h5\nReading subject 7, action Discussion\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Discussion*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Discussion 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Discussion.h5\nReading subject 7, action Eating\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Eating*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Eating 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Eating.h5\nReading subject 7, action Greeting\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Greeting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Greeting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Greeting.h5\nReading subject 7, action Phoning\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Phoning*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Phoning.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Phoning 2.h5\nReading subject 7, action Photo\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Photo*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Photo 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Photo.h5\nReading subject 7, action Posing\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Posing*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Posing 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Posing.h5\nReading subject 7, action Purchases\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Purchases*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Purchases 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Purchases.h5\nReading subject 7, action Sitting\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Sitting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Sitting.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Sitting 1.h5\nReading subject 7, action SittingDown\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/SittingDown*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/SittingDown 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/SittingDown.h5\nReading subject 7, action Smoking\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Smoking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Smoking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Smoking 1.h5\nReading subject 7, action Waiting\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Waiting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Waiting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Waiting 2.h5\nReading subject 7, action WalkDog\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkDog*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkDog 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkDog.h5\nReading subject 7, action Walking\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Walking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Walking 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/Walking 2.h5\nReading subject 7, action WalkTogether\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkTogether*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkTogether 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S7/MyPoses/3D_positions/WalkTogether.h5\nReading subject 8, action Directions\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Directions*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Directions 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Directions.h5\nReading subject 8, action Discussion\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Discussion*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Discussion 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Discussion.h5\nReading subject 8, action Eating\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Eating*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Eating 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Eating.h5\nReading subject 8, action Greeting\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Greeting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Greeting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Greeting.h5\nReading subject 8, action Phoning\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Phoning*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Phoning.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Phoning 1.h5\nReading subject 8, action Photo\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Photo*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Photo 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Photo.h5\nReading subject 8, action Posing\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Posing*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Posing 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Posing.h5\nReading subject 8, action Purchases\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Purchases*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Purchases 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Purchases.h5\nReading subject 8, action Sitting\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Sitting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Sitting.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Sitting 1.h5\nReading subject 8, action SittingDown\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/SittingDown*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/SittingDown 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/SittingDown.h5\nReading subject 8, action Smoking\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Smoking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Smoking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Smoking 1.h5\nReading subject 8, action Waiting\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Waiting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Waiting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Waiting.h5\nReading subject 8, action WalkDog\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkDog*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkDog 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkDog.h5\nReading subject 8, action Walking\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Walking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Walking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/Walking 1.h5\nReading subject 8, action WalkTogether\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkTogether*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkTogether 2.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S8/MyPoses/3D_positions/WalkTogether 1.h5\nReading subject 9, action Directions\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Directions*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Directions 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Directions.h5\nReading subject 9, action Discussion\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Discussion*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Discussion 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Discussion 2.h5\nReading subject 9, action Eating\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Eating*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Eating 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Eating.h5\nReading subject 9, action Greeting\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Greeting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Greeting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Greeting.h5\nReading subject 9, action Phoning\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Phoning*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Phoning.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Phoning 1.h5\nReading subject 9, action Photo\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Photo*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Photo 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Photo.h5\nReading subject 9, action Posing\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Posing*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Posing 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Posing.h5\nReading subject 9, action Purchases\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Purchases*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Purchases 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Purchases.h5\nReading subject 9, action Sitting\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Sitting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Sitting.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Sitting 1.h5\nReading subject 9, action SittingDown\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/SittingDown*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/SittingDown 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/SittingDown.h5\nReading subject 9, action Smoking\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Smoking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Smoking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Smoking 1.h5\nReading subject 9, action Waiting\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Waiting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Waiting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Waiting.h5\nReading subject 9, action WalkDog\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkDog*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkDog 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkDog.h5\nReading subject 9, action Walking\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Walking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Walking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/Walking 1.h5\nReading subject 9, action WalkTogether\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkTogether*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkTogether 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S9/MyPoses/3D_positions/WalkTogether.h5\nReading subject 11, action Directions\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Directions*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Directions 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Directions.h5\nReading subject 11, action Discussion\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Discussion*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Discussion 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Discussion 2.h5\nReading subject 11, action Eating\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Eating*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Eating 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Eating.h5\nReading subject 11, action Greeting\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Greeting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Greeting 2.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Greeting.h5\nReading subject 11, action Phoning\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Phoning*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Phoning 3.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Phoning 2.h5\nReading subject 11, action Photo\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Photo*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Photo 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Photo.h5\nReading subject 11, action Posing\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Posing*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Posing 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Posing.h5\nReading subject 11, action Purchases\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Purchases*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Purchases 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Purchases.h5\nReading subject 11, action Sitting\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Sitting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Sitting.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Sitting 1.h5\nReading subject 11, action SittingDown\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/SittingDown*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/SittingDown 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/SittingDown.h5\nReading subject 11, action Smoking\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Smoking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Smoking 2.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Smoking.h5\nReading subject 11, action Waiting\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Waiting*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Waiting 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Waiting.h5\nReading subject 11, action WalkDog\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkDog*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkDog 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkDog.h5\nReading subject 11, action Walking\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Walking*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Walking.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/Walking 1.h5\nReading subject 11, action WalkTogether\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkTogether*.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkTogether 1.h5\n../../Implementations/GenMultiHypothesis/data/h36m/S11/MyPoses/3D_positions/WalkTogether.h5\n"
    }
   ],
   "source": [
    "TRAIN_SUBJECTS = [1,5,6,7,8]\n",
    "TEST_SUBJECTS  = [9,11]\n",
    "\n",
    "actions = define_actions(FLAGS['action'])\n",
    "\n",
    "orig_h36m_train_3d = load_h36m_dataset('../../Implementations/GenMultiHypothesis/data/h36m/', TRAIN_SUBJECTS, actions, 3)\n",
    "orig_h36m_test_3d = load_h36m_dataset('../../Implementations/GenMultiHypothesis/data/h36m/', TEST_SUBJECTS, actions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2356, 96)\n"
    }
   ],
   "source": [
    "print(orig_h36m_test_3d[(9, 'Directions', 'Directions 1.h5')].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcams = cameras.load_cameras('data/h36m_cameras.h5')\n",
    "\n",
    "orig_h36m_train_cam = transform_world_to_camera(orig_h36m_train_3d, rcams)\n",
    "\n",
    "orig_h36m_train_cam, _ = postprocess_3d(orig_h36m_train_cam)\n",
    "\n",
    "complete_train = copy.deepcopy(np.vstack(orig_h36m_train_cam.values()))\n",
    "\n",
    "mean_3d, std_3d, dti_3d, dtu_3d = normalization_stats(complete_train, 3)\n",
    "\n",
    "# orig_h36m_train_cam = normalize_data(complete_train, mean_3d, std_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2356, 96)\n(2356, 96)\n"
    }
   ],
   "source": [
    "orig_h36m_test_cam = transform_world_to_camera(orig_h36m_test_3d, rcams)\n",
    "\n",
    "print(orig_h36m_test_cam[(9, 'Directions', 'Directions 1.54138969.h5')].shape)\n",
    "\n",
    "orig_h36m_test_cam, _ = postprocess_3d(orig_h36m_test_cam)\n",
    "\n",
    "complete_test = copy.deepcopy(np.vstack(orig_h36m_test_cam.values()))\n",
    "\n",
    "print(orig_h36m_test_cam[(9, 'Directions', 'Directions 1.54138969.h5')].shape)\n",
    "\n",
    "orig_h36m_test_cam = normalize_data(orig_h36m_test_cam, mean_3d, std_3d, dtu_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_h36m_train_2d = project_to_cameras(orig_h36m_train_3d, rcams)\n",
    "\n",
    "complete_train_2d = copy.deepcopy(np.vstack(orig_h36m_train_2d.values()))\n",
    "\n",
    "mean_2d, std_2d, dti_2d, dtu_2d = normalization_stats(complete_train_2d, dim=2)\n",
    "\n",
    "# orig_h36m_train_2d = normalize_data(complete_train_2d, mean_2d, std_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_h36m_test_2d = project_to_cameras(orig_h36m_test_3d, rcams)\n",
    "\n",
    "orig_h36m_test_2d = normalize_data(orig_h36m_test_2d, mean_2d, std_2d, dtu_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_select = [0, 1, 2, 3, 6, 7, 8, 12, 13, 15, 17, 18, 19, 25, 26, 27]\n",
    "\n",
    "mod_2d = orig_h36m_test_2d[(9, 'Directions', 'Directions 1.54138969.h5')].reshape((-1, 32, 2))\n",
    "mod_2d = mod_2d[:, indices_to_select, :]\n",
    "\n",
    "mod_3d = orig_h36m_test_cam[(9, 'Directions', 'Directions 1.54138969.h5')].reshape((-1, 32, 3))\n",
    "mod_3d = mod_3d[:, indices_to_select, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_mu_2d = mean_2d.reshape((32, 2))[indices_to_select, :].reshape((32))\n",
    "mod_mu_3d = mean_3d.reshape((32, 3))[indices_to_select, :].reshape((48))\n",
    "mod_std_2d = std_2d.reshape((32, 2))[indices_to_select, :].reshape((32))\n",
    "mod_std_3d = std_3d.reshape((32, 3))[indices_to_select, :].reshape((48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating 2 bi-layers of 1024 units.\n199.0036622693718\n"
    }
   ],
   "source": [
    "evaluate(\n",
    "    mod_2d.reshape((-1, 32)),\n",
    "    mod_3d.reshape((-1, 48)),\n",
    "    [mod_mu_2d, mod_mu_3d],\n",
    "    [mod_std_2d, mod_std_3d]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Directions   43.10767701930445\nDiscussion   51.05013986771212\nEating   51.724349459074\nGreeting   49.84631152856204\nPhoning   57.32854292441411\nPhoto   64.59227430961076\nPosing   47.56292327853188\nPurchases   47.49338592765643\nSitting   64.01063001842849\nSittingDown   71.56349413179795\nSmoking   55.90719920972362\nWaiting   50.81513474262019\nWalkDog   57.66722607785159\nWalking   41.08270919073637\nWalkTogether   44.42559820420118\n53.21183972601502\n"
    }
   ],
   "source": [
    "evaluate_orig(\n",
    "    orig_h36m_test_2d, orig_h36m_test_cam,\n",
    "    mean_2d, mean_3d,\n",
    "    std_2d, std_3d,\n",
    "    dtu_2d, dtu_3d,\n",
    "    dti_2d, dti_3d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpy36tf114condaec0b5fab3db54cdd8cb0163190e5d86e",
   "display_name": "Python 3.6.10 64-bit ('py36_tf114': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}